{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from scipy.misc import imread\n",
    "get_ipython().magic('matplotlib inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activation\n",
    "\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "train=pd.read_csv(cwd + \"/train.csv\")\n",
    "test=pd.read_csv(cwd + \"/train.csv\")\n",
    "train_path = cwd + \"/data/train/\"\n",
    "test_path = cwd + \"/data/train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import imresize\n",
    "\n",
    "train_img=[]\n",
    "for i in range(len(train)):\n",
    "    temp_img = image.load_img(train_path + train['filename'][i], target_size=(224, 224))\n",
    "    temp_img = image.img_to_array(temp_img)\n",
    "    train_img.append(temp_img)\n",
    "\n",
    "train_img=np.array(train_img) \n",
    "train_img=preprocess_input(train_img)\n",
    "\n",
    "test_img=[]\n",
    "for i in range(len(test)):\n",
    "    temp_img=image.load_img(test_path+test['filename'][i],target_size=(224,224))\n",
    "    temp_img=image.img_to_array(temp_img)\n",
    "    test_img.append(temp_img)\n",
    "\n",
    "test_img=np.array(test_img) \n",
    "test_img=preprocess_input(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "def vgg16_model(img_rows, img_cols, channel=1, num_classes=None):\n",
    "\n",
    "    model = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "    model.layers.pop()\n",
    "\n",
    "    model.outputs = [model.layers[-1].output]\n",
    "\n",
    "    model.layers[-1].outbound_nodes = []\n",
    "\n",
    "    x=Dense(num_classes, activation='softmax')(model.output)\n",
    "\n",
    "    model=Model(model.input,x)\n",
    "    \n",
    "    #To set the first 8 layers to non-trainable (weights will not be updated)\n",
    "    for layer in model.layers[:8]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Learning rate is changed to 0.001\n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y=np.asarray(train['label'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "train_y = le.fit_transform(train_y)\n",
    "\n",
    "train_y=to_categorical(train_y)\n",
    "\n",
    "train_y=np.array(train_y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, Y_train, Y_valid=train_test_split(train_img,train_y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to fine-tune on 3000 samples from Cifar10\n",
    "img_rows, img_cols = 224, 224 # Resolution of inputs\n",
    "channel = 3\n",
    "num_classes = 10 \n",
    "batch_size = 16\n",
    "nb_epoch = 10\n",
    "\n",
    "# Load our model\n",
    "model = vgg16_model(img_rows, img_cols, channel, num_classes)\n",
    "\n",
    "model.summary()\n",
    "# Start Fine-tuning\n",
    "model.fit(X_train, Y_train,batch_size=batch_size,epochs=nb_epoch,shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions_valid = model.predict(X_valid, batch_size=batch_size, verbose=1)\n",
    "\n",
    "# Cross-entropy loss score\n",
    "score = log_loss(Y_valid, predictions_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
